{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQYFHTQErg_i"
   },
   "source": [
    "Google Colab Specific loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "x5twmjxCrhiN",
    "outputId": "5b5ff470-ae70-4749-94d9-c571e48b4712",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'FNB_grading/': No such file or directory\n",
      "Cloning into 'FNB_grading'...\n",
      "remote: Counting objects: 59, done.\u001b[K\n",
      "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
      "remote: Total 59 (delta 23), reused 53 (delta 17), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (59/59), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "!rm -r FNB_grading/\n",
    "!rm preprocessing.py\n",
    "!git clone https://git@github.com/justinwhatley/FNB_grading.git\n",
    "!mv FNB_grading/preprocessing.py preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T98hI6QTo25d"
   },
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "-6ndE2n3xMmd",
    "outputId": "7c442ea8-e9ab-4555-e540-233d0aa7d9f0"
   },
   "outputs": [],
   "source": [
    "# Installing dependencies\n",
    "!pip install PyDrive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FH72DYwrxesw"
   },
   "outputs": [],
   "source": [
    "# Authorizing Google SDK\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYMo_Nap7Pbv"
   },
   "outputs": [],
   "source": [
    "# Downloads raw data from Google Drive and extracts it\n",
    "!rm -rf FNAB_raw\n",
    "download = drive.CreateFile({'id': '10aVomUhZyaMiJuqfSBEzcwWVKkzE7hBN'})\n",
    "download.GetContentFile('FNAB_raw.tar.gz')\n",
    "!tar -xzf FNAB_raw.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIiH3i19x_yn"
   },
   "outputs": [],
   "source": [
    "# Downloads preprocessed data from Google Drive and extracts it\n",
    "!rm -rf FNAB_preprocessed\n",
    "download = drive.CreateFile({'id': '1Amc6q-XfcErdimjPxtuAGo_jpDay0aqw'})\n",
    "download.GetContentFile('FNAB.preprocessed.tar.gz')\n",
    "!tar -xzf FNAB.preprocessed.tar.gz;\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "eFSKRkc_rtwt",
    "outputId": "74c35034-37c1-4824-ee0d-703f956a315c"
   },
   "outputs": [],
   "source": [
    "# Sets the path based on the environment being used\n",
    "\n",
    "def get_path(computer_str):\n",
    "    \"\"\"\n",
    "    Returns the correct path based on where the program is run\n",
    "    \"\"\"\n",
    "\n",
    "    mac_data_path = '/Users/justinwhatley/Dropbox/FevensLab'\n",
    "    linux_data_path = '/home/justin/Dropbox/FevensLab'\n",
    "    \n",
    "    if computer_str.lower() == 'mac':\n",
    "        return os.path.join(mac_data_path)\n",
    "\n",
    "    elif computer_str.lower() == 'linux': \n",
    "        return os.path.join(linux_data_path)\n",
    "\n",
    "    elif computer_str.lower() == 'colab':\n",
    "        # Google colab path\n",
    "        return os.path.join('')\n",
    "\n",
    "    else: \n",
    "        print('Incorrect base path option')\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCvEVe5Go6nH"
   },
   "source": [
    "Separates dataset into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OoO2f7Xs6W9"
   },
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "xhklMauLsalV",
    "outputId": "41265bec-9e69-4f25-9e65-513ea653eb18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data from path: /home/justin/Dropbox/FevensLab/FNAB_raw\n",
      "82\n",
      "130\n",
      "Bin size: 4596\n",
      "Bin size: 4314\n",
      "Bin size: 4320\n",
      "Bin size: 4374\n",
      "Bin size: 4500\n",
      "Bin size: 919\n",
      "Bin size: 1015\n",
      "Bin size: 761\n",
      "Bin size: 769\n",
      "Bin size: 1012\n",
      "82\n",
      "24\n",
      "Bin size: 92\n",
      "Bin size: 92\n",
      "Bin size: 92\n",
      "Bin size: 95\n",
      "Bin size: 97\n",
      "Bin size: 20\n",
      "Bin size: 26\n",
      "Bin size: 20\n",
      "Bin size: 22\n",
      "Bin size: 18\n",
      "Removing directory: /home/justin/Dropbox/FevensLab/training_validation_dataset\n",
      "Taking files from input directory: /home/justin/Dropbox/FevensLab/FNAB_preprocessed/original_data/MG2\n",
      "Storing them in output directory: /home/justin/Dropbox/FevensLab/training_validation_dataset/original_data/training/MG2\n",
      "Taking files from input directory: /home/justin/Dropbox/FevensLab/FNAB_preprocessed/original_data/MG2\n",
      "Storing them in output directory: /home/justin/Dropbox/FevensLab/training_validation_dataset/original_data/validation/MG2\n",
      "Taking files from input directory: /home/justin/Dropbox/FevensLab/FNAB_preprocessed/original_data/MG3\n",
      "Storing them in output directory: /home/justin/Dropbox/FevensLab/training_validation_dataset/original_data/training/MG3\n",
      "Taking files from input directory: /home/justin/Dropbox/FevensLab/FNAB_preprocessed/original_data/MG3\n",
      "Storing them in output directory: /home/justin/Dropbox/FevensLab/training_validation_dataset/original_data/validation/MG3\n",
      "Finished! \n"
     ]
    }
   ],
   "source": [
    "!rm -rf training_validation_dataset/\n",
    "\n",
    "base_directory = get_path('linux')\n",
    "raw_data_directory_path = os.path.join(base_directory, 'FNAB_raw')\n",
    "preprocessed_directory_path = os.path.join(base_directory, 'FNAB_preprocessed')\n",
    "training_validation_path = os.path.join(base_directory, 'training_validation_dataset')\n",
    "\n",
    "class_keyword_1 = 'MG2'\n",
    "class_keyword_2 = 'MG3'\n",
    "classes_list = [class_keyword_1, class_keyword_2]\n",
    "\n",
    "# Prepares data\n",
    "height, width = 224, 224\n",
    "\n",
    "files_list_by_class = preprocessing.get_raw_file_list(raw_data_directory_path, classes_list)\n",
    "preprocessing.create_preprocessed_directory(classes_list, files_list_by_class, preprocessed_directory_path, height, width, overwrite_previous_preprocessed_data = False)\n",
    "\n",
    "# Gets patch file data\n",
    "patched_class_file_list = preprocessing.get_data_by_class(os.path.join(preprocessed_directory_path, 'patched_data'), classes_list)\n",
    "original_class_file_list = preprocessing.get_data_by_class(os.path.join(preprocessed_directory_path, 'original_data'), classes_list)\n",
    "\n",
    "# Separate data into k-folds\n",
    "number_of_folds = 5\n",
    "files_per_fold =  [4000, 700]\n",
    "patched_files_in_folds = preprocessing.separate_into_k_folds(number_of_folds, patched_class_file_list, files_per_fold)\n",
    "# print(patched_files_in_folds)\n",
    "\n",
    "files_per_fold =  [90, 20]\n",
    "original_files_in_folds = preprocessing.separate_into_k_folds(number_of_folds, original_class_file_list, files_per_fold)\n",
    "# print(original_files_in_folds)\n",
    "\n",
    "# Iterates through folds\n",
    "\n",
    "for i in range(number_of_folds):   \n",
    "    validation_fold = i\n",
    "    \n",
    "    # Removes previous training and validation directories\n",
    "    preprocessing.remove_dir(training_validation_path)\n",
    "                                       \n",
    "    # Selects a validation and training set on the original data\n",
    "    preprocessing.assign_folds_to_training_and_validation(preprocessed_directory_path, training_validation_path, classes_list, original_files_in_folds, validation_fold, type = 'original_data')\n",
    "\n",
    "    # Selects a validation and training set on the patched data\n",
    "#     preprocessing.assign_folds_to_training_and_validation(preprocessed_directory_path, training_validation_path, classes_list, patched_files_in_folds, validation_fold, type = 'patched_data')\n",
    "\n",
    "    # TODO call training and validation from here\n",
    "    print('Finished! ')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEGqRMFLBhky"
   },
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRCXDARNNs2u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os.path as path\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-ocYvTQ1BnIm",
    "outputId": "315de44c-79ef-44ab-aba3-f3837e01e07d"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-a28e9f191fcb>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-a28e9f191fcb>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ls -l FNAB_preprocessed/original_data/MG2/ | egrep -c '^-'\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TODO figure out why bins are artificially double the size\n",
    "ls -l FNAB_preprocessed/original_data/MG2/ | egrep -c '^-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "p0bF59XOJcRU",
    "outputId": "da34326d-862c-4817-d932-95b1d0e9395f"
   },
   "outputs": [],
   "source": [
    "# TODO figure out why there are incorrect (small) numbers of files in the different training and validation sets\n",
    "ls -l training_validation_dataset/patched_data/training/MG3 | egrep -c '^-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KltowymNR5u"
   },
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "\n",
    "\n",
    "# Dimensions images will be resized to for processing\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "class_list = ['MG2', 'MG3']\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Yz2JBxyoNW3W",
    "outputId": "a44b3b68-1534-4434-a859-aa8f1339d009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 438\n",
      "Number of validation samples: 110\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Calculates the number of training and validation samples\"\"\"\n",
    "# dataset_name = 'training_validation_dataset/patched_data'\n",
    "\n",
    "# Loads the unaltered images that have been place in training and validation datasets \n",
    "# dataset_name = os.path.join(training_validation_path, 'patched_data')\n",
    "dataset_name = os.path.join(training_validation_path, 'original_data')\n",
    "train_data_dir = path.join(dataset_name, 'training')\n",
    "validation_data_dir = path.join(dataset_name, 'validation')\n",
    "\n",
    "# Calculate number of files in directory \n",
    "import os\n",
    "import glob\n",
    "\n",
    "def get_file_list(path_regex):\n",
    "    file_list = []\n",
    "    for filename in glob.glob(path_regex): \n",
    "        file_list.append(filename)\n",
    "    return file_list\n",
    "  \n",
    "# Loads jpg files using regex\n",
    "path_regex = path.join(train_data_dir + '/*/*.jpg')\n",
    "nb_train_samples = len(get_file_list(path_regex))\n",
    "print('Number of training samples: '+ str(nb_train_samples))\n",
    "\n",
    "path_regex = path.join(validation_data_dir + '/*/*.jpg')\n",
    "nb_validation_samples = len(get_file_list(path_regex))\n",
    "print('Number of validation samples: '+ str(nb_validation_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EShr1z8SM96G"
   },
   "outputs": [],
   "source": [
    "\"\"\" Defines the model \"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justin/anaconda3/lib/python3.6/site-packages/keras/activations.py:197: UserWarning: Do not pass a layer instance (such as Dense) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "# Laya's first CNN\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), input_shape=input_shape))\n",
    "model.add(Activation(Dense(128, activation='relu')))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), strides=(1, 1)))\n",
    "model.add(Activation(Dense(128, activation='relu')))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1)))\n",
    "model.add(Activation(Dense(64, activation='relu')))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1)))\n",
    "model.add(Activation(Dense(64, activation='relu')))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1)))\n",
    "model.add(Activation(Dense(32, activation='relu')))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1)))\n",
    "model.add(Activation(Dense(32, activation='relu')))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YgnkP4Ovduj_"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-947532fe9aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracyHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m history = model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m     11\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        \n",
    "history = AccuracyHistory()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1697
    },
    "colab_type": "code",
    "id": "-MkLNJlEOc-q",
    "outputId": "1b0029b8-1733-4dc2-ae95-c836b263d2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 438 images belonging to 2 classes.\n",
      "Found 110 images belonging to 2 classes.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_91 to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-d1865ac7bfd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     validation_steps= nb_validation_samples // batch_size)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first_try.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_91 to have shape (2,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "\"\"\" Training and validation for regular network \"\"\"\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=False)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps= nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6324
    },
    "colab_type": "code",
    "id": "5RwrYbPKf_8j",
    "outputId": "dfe62f88-e0c1-486f-dab2-ec70067fa2a8"
   },
   "outputs": [],
   "source": [
    "ls training_validation_dataset/patched_data/training/MG3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model CNN:  accuracy history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "# plt.grid(b=False)\n",
    "# plt.savefig('accuracy.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model CNN:  loss history')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "# plt.grid(b=False, )\n",
    "# plt.savefig('loss.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('loss.png')\n",
    "# files.download('accuracy.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fnab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
